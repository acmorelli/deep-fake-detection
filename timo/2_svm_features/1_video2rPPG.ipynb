{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e1c626a",
   "metadata": {},
   "source": [
    "### rPPG detector (GRGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38a84928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1726160194.250971 29658749 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M1\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import zip_longest\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Initialize MediaPipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=False,\n",
    "    max_num_faces=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "def extract_filename(file_path):\n",
    "    # filename pattern (e.g., \"002_006\")\n",
    "    match = re.search(r'([^/]+)\\.mp4$', file_path)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_videos_info(root_folder):\n",
    "    video_info_list = []\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            if file.endswith(('.mp4', '.mkv', '.avi', '.mov')):\n",
    "                file_path = os.path.join(root, file)\n",
    "                video_info_list.append(file_path)\n",
    "    return video_info_list\n",
    "\n",
    "def save_rppg_signal(signal, length, dataset, compression, amount_videos, category, filename):\n",
    "        with open(f'2_rPPG/{dataset}_{compression}_{str(amount_videos)}_{length}/{category}/{filename}.csv', mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(signal)\n",
    "\n",
    "\n",
    "'''\n",
    "USAGE:\n",
    "\n",
    "• Put the desired video dataset into the 1_video_input folder\n",
    "• Specify the parameters below\n",
    "• create_folders will build all necessary directories\n",
    "\n",
    "'''\n",
    "\n",
    "dataset = 'faceshifter'\n",
    "compression = 'raw'\n",
    "amount_videos = 200 #len(get_videos_info('1_video_input/'))\n",
    "category = 'original' #'original' #'manipulated'\n",
    "loop_video = False \n",
    "\n",
    "create_folders = False\n",
    "\n",
    "# Create necessary folders if they don't exist\n",
    "if create_folders:\n",
    "    subfolders = ['','/original','/manipulated','/stats']\n",
    "    for i in subfolders:\n",
    "        # os.makedirs(f'2_rPPG/{dataset}_{compression}_{str(amount_videos)}_5s'+i, exist_ok=True)\n",
    "        # os.makedirs(f'2_rPPG/{dataset}_{compression}_{str(amount_videos)}_10s'+i, exist_ok=True)\n",
    "        # os.makedirs(f'2_rPPG/{dataset}_{compression}_{str(amount_videos)}_20s'+i, exist_ok=True)\n",
    "        os.makedirs(f'2_rPPG/{dataset}_{compression}_{str(amount_videos)}_inf'+i, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28119b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1726160194.261776 29658858 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1726160194.263831 29658853 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "/Users/timoschmitz/Library/Python/3.9/lib/python/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "2024-09-12 18:56:34.456 Python[93019:29658749] WARNING: Secure coding is automatically enabled for restorable state! However, not on all supported macOS versions of this application. Opt-in to secure coding explicitly by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState:.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "'''\n",
    "Program to extract specific ROIs from the video frame. \n",
    "Compute the average values of color channels for the selected ROIs.\n",
    "Mediapipe library is used to detect the face landmarks\n",
    "'''\n",
    "\n",
    "#ROI loction points according to the paper by F.Haugg et al. (added additional points in each case)\n",
    "landmark_head = [107, 66, 69, 67, 109, 10, 338, 297, 299, 296, 336, 9]\n",
    "landmark_lcheek = [118, 119, 120, 47, 126, 209, 49, 129, 203, 205, 50]\n",
    "landmark_rcheek = [347, 348, 349, 277, 355, 429, 279, 358, 423, 425, 280]\n",
    "\n",
    "video_list = get_videos_info('1_video_input/')\n",
    "\n",
    "for i in video_list:\n",
    "    filename = extract_filename(i)\n",
    "    cap = cv2.VideoCapture(i)\n",
    "\n",
    "    # Get the frames per second (fps) of the video\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    # Get the total number of frames in the video\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    # Limit frames to specified maximal duration in seconds\n",
    "    max_frames = total_frames\n",
    "    #max_frames = max_duration * fps  # Number of frames\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file.\")\n",
    "        exit()\n",
    "\n",
    "    #Initialise the list of color channels\n",
    "    avg_b = []\n",
    "    avg_g = []\n",
    "    avg_r = []\n",
    "\n",
    "    gr_signal = []\n",
    "    gb_signal = []\n",
    "    grgb_signal = []\n",
    "\n",
    "    frame_count = 0  # Initialize frame counter\n",
    "    while frame_count < max_frames:\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            cv2.waitKey(1)\n",
    "            break\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            if loop_video:\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "                ret, frame = cap.read()  # Reads a new frame\n",
    "            else:  # Ends video playback\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                cv2.waitKey(1)\n",
    "                break\n",
    "\n",
    "        # Convert the frame to RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the frame with MediaPipe Face Mesh\n",
    "        results = face_mesh.process(rgb_frame)\n",
    "\n",
    "        # Extract the values of specific ROI with respect to landmark position points\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                # Extracting specific landmark coordinates\n",
    "                h, w, _ = frame.shape\n",
    "                landmark_coords_head = []\n",
    "                landmark_coords_lcheek = []\n",
    "                landmark_coords_rcheek = []\n",
    "\n",
    "                for i, j, k in zip_longest(landmark_head, landmark_lcheek, landmark_rcheek, fillvalue = None):\n",
    "\n",
    "                    if i is not None:\n",
    "                        landmark_h = face_landmarks.landmark[i]\n",
    "                        x = int(landmark_h.x * w)\n",
    "                        y = int(landmark_h.y * h)\n",
    "                        landmark_coords_head.append((x, y))\n",
    "                        #cv2.circle(frame, (x, y), radius=1, color=(0, 0, 0), thickness=1)\n",
    "                    \n",
    "                    if j is not None:\n",
    "\n",
    "                        landmark_lc = face_landmarks.landmark[j]\n",
    "                        x1 = int(landmark_lc.x * w)\n",
    "                        y1 = int(landmark_lc.y * h)\n",
    "                        landmark_coords_lcheek.append((x1, y1))\n",
    "                        #cv2.circle(frame, (x1, y1), radius=1, color=(0, 0, 0), thickness=1)\n",
    "                    \n",
    "                    if k is not None:\n",
    "\n",
    "                        landmark_rc = face_landmarks.landmark[k]\n",
    "                        x2 = int(landmark_rc.x * w)\n",
    "                        y2 = int(landmark_rc.y * h)\n",
    "                        landmark_coords_rcheek.append((x2, y2))\n",
    "                        #cv2.circle(frame, (x2, y2), radius=1, color=(0, 0, 0), thickness=1)\n",
    "                    \n",
    "                # Determine the bounding box around the specified landmarks -\n",
    "                # Forehead\n",
    "                x_coords, y_coords = zip(*landmark_coords_head)\n",
    "                x_min, x_max = max(0, min(x_coords)), min(w, max(x_coords))\n",
    "                y_min, y_max = max(0, min(y_coords)), min(h, max(y_coords))\n",
    "                # ROI\n",
    "                forehead_roi = frame[y_min:y_max, x_min:x_max]\n",
    "                forehead_blue = np.mean(forehead_roi[:, :, 0])\n",
    "                forehead_green = np.mean(forehead_roi[:, :, 1])\n",
    "                forehead_red = np.mean(forehead_roi[:, :, 2])\n",
    "\n",
    "                #lcheek\n",
    "                x_coords, y_coords = zip(*landmark_coords_lcheek)\n",
    "                x_min, x_max = max(0, min(x_coords)), min(w, max(x_coords))\n",
    "                y_min, y_max = max(0, min(y_coords)), min(h, max(y_coords))\n",
    "                # ROI\n",
    "                lcheek_roi = frame[y_min:y_max, x_min:x_max]\n",
    "                lcheek_blue = np.mean(lcheek_roi[:, :, 0])\n",
    "                lcheek_green = np.mean(lcheek_roi[:, :, 1])\n",
    "                lcheek_red = np.mean(lcheek_roi[:, :, 2])\n",
    "\n",
    "                #rcheek\n",
    "                x_coords, y_coords = zip(*landmark_coords_rcheek)\n",
    "                x_min, x_max = max(0, min(x_coords)), min(w, max(x_coords))\n",
    "                y_min, y_max = max(0, min(y_coords)), min(h, max(y_coords))\n",
    "                # ROI\n",
    "                rcheek_roi = frame[y_min:y_max, x_min:x_max]\n",
    "                rcheek_blue = np.mean(rcheek_roi[:, :, 0])\n",
    "                rcheek_green = np.mean(rcheek_roi[:, :, 1])\n",
    "                rcheek_red = np.mean(rcheek_roi[:, :, 2])\n",
    "\n",
    "                # Crop the image\n",
    "                #cropped_image = frame[y_min:y_max, x_min:x_max]\n",
    "                #average = np.mean(forehead_roi[:, :, 0], lcheek_roi[:, :, 0], rcheek_roi[:, :, 0])\n",
    "                #print(forehead_red, lcheek_red, rcheek_red)\n",
    "                \n",
    "                avg_r.append(np.mean([forehead_red, lcheek_red, rcheek_red]))\n",
    "                avg_g.append(np.mean([forehead_green, lcheek_green, rcheek_green]))\n",
    "                avg_b.append(np.mean([forehead_blue, lcheek_blue, rcheek_blue]))\n",
    "\n",
    "                '''\n",
    "                # Display the cropped image in a separate window\n",
    "                cv2.namedWindow(\"Cropped_Image\", cv2.WINDOW_NORMAL) \n",
    "                cv2.resizeWindow(\"Cropped_Image\", 400,200)\n",
    "                cv2.imshow('Cropped_Image', cropped_image)\n",
    "                '''\n",
    "        # Display the frame\n",
    "        cv2.namedWindow(\"rPPG\", cv2.WINDOW_NORMAL) \n",
    "        cv2.resizeWindow(\"rPPG\", 900,650)\n",
    "        cv2.imshow(\"rPPG\", frame)\n",
    "        frame_count += 1  # Increment the frame counter\n",
    "\n",
    " \n",
    "    # Calculate the GRGB values of the frames\n",
    "    # Implementation of the method mentioned in paper by F.Haugg et al.\n",
    "    gr_signal = [a/b for a, b in zip(avg_g, avg_r)]\n",
    "    gb_signal = [a/b for a, b in zip(avg_g, avg_b)]\n",
    "    grgb_signal = [a + b for a , b in zip(gr_signal, gb_signal)]\n",
    "\n",
    "\n",
    "    # Save all versions of rPPG\n",
    "    time_limits = ['inf'] #5, 10, 20, \n",
    "    max_frames_dict = {t: (t * fps if isinstance(t, int) else total_frames) for t in time_limits}\n",
    "    for length, max_frames in max_frames_dict.items():\n",
    "        truncated_signal = grgb_signal[:int(max_frames)]  # Slice signal based on length\n",
    "        save_rppg_signal(truncated_signal, f'{length}s' if isinstance(length, int) else 'inf', dataset, compression, amount_videos, category, filename)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "face_mesh.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
